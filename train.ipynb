{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyjuice as juice\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pyjuice.nodes.distributions as dists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5008, 805)\n",
      "torch.Size([4006, 805])\n",
      "torch.Size([1002, 805])\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = torchvision.datasets.MNIST(root = \"./data\", train = True, download = True)\n",
    "# valid_dataset = torchvision.datasets.MNIST(root = \"./data\", train = False, download = True)\n",
    "\n",
    "# train_data = train_dataset.data.reshape(60000, 28*28)\n",
    "# valid_data = valid_dataset.data.reshape(10000, 28*28)\n",
    "\n",
    "data_dir = \"genetic_pc/data1kg\"\n",
    "file_name = \"805.data\"\n",
    "\n",
    "def load(file_name, data_dir):\n",
    "    filename = f\"{data_dir}/{file_name}\"\n",
    "    dataframe = pd.read_csv(filename, header=None,\n",
    "                            true_values=[\"1\", \"2\", \"3\"],\n",
    "                            false_values=[\"0\"], dtype=object)\n",
    "    data = dataframe.iloc[:, 0].str.split(' ')\n",
    "    return np.array([np.array(entry) for entry in data])\n",
    "\n",
    "data = load(file_name, data_dir)\n",
    "data = data.astype(np.int8)\n",
    "print(data.shape)\n",
    "\n",
    "total_size = len(data)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "indices = np.random.permutation(total_size)\n",
    "train_indices, valid_indices = indices[:train_size], indices[train_size:]\n",
    "train_data, valid_data = data[train_indices], data[valid_indices]\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.uint8)\n",
    "valid_data = torch.tensor(valid_data, dtype=torch.uint8)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = TensorDataset(train_data),\n",
    "    batch_size = 512,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset = TensorDataset(valid_data),\n",
    "    batch_size = 512,\n",
    "    shuffle = False,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling 8 TensorCircuit layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# The data is required to construct the backbone Chow-Liu Tree structure for the HCLT\n",
    "ns = juice.structures.HCLT(\n",
    "    train_data.float().to(device),\n",
    "    num_latents = 128\n",
    ")\n",
    "\n",
    "pc = juice.compile(ns)\n",
    "pc.to(device)\n",
    "\n",
    "optimizer = juice.optim.CircuitOptimizer(pc, lr = 0.1, pseudocount = 0.1, method = \"EM\")\n",
    "scheduler = juice.optim.CircuitScheduler(\n",
    "    optimizer,\n",
    "    method = \"multi_linear\",\n",
    "    lrs = [0.9, 0.1, 0.05],\n",
    "    milestone_steps = [0, len(train_loader) * 100, len(train_loader) * 350]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x = batch[0].to(device)\n",
    "\n",
    "    lls = pc(x, record_cudagraph = True)\n",
    "    lls.mean().backward()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/350][train LL: -1292.32; val LL: -404.04].....[train forward+backward+step 0.77; val forward 0.00] \n",
      "[Epoch 2/350][train LL: -402.48; val LL: -401.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 3/350][train LL: -400.74; val LL: -400.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 4/350][train LL: -399.57; val LL: -400.04].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 5/350][train LL: -398.94; val LL: -399.46].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 6/350][train LL: -398.36; val LL: -398.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 7/350][train LL: -398.35; val LL: -398.64].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 8/350][train LL: -397.81; val LL: -398.19].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 9/350][train LL: -397.83; val LL: -398.25].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 10/350][train LL: -397.63; val LL: -397.95].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 11/350][train LL: -397.17; val LL: -397.62].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 12/350][train LL: -397.53; val LL: -397.64].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 13/350][train LL: -396.93; val LL: -397.40].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 14/350][train LL: -396.79; val LL: -397.54].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 15/350][train LL: -396.67; val LL: -397.07].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 16/350][train LL: -396.59; val LL: -396.89].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 17/350][train LL: -396.37; val LL: -397.11].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 18/350][train LL: -396.05; val LL: -396.84].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 19/350][train LL: -396.56; val LL: -396.65].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 20/350][train LL: -396.05; val LL: -396.61].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 21/350][train LL: -395.91; val LL: -396.65].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 22/350][train LL: -396.01; val LL: -396.44].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 23/350][train LL: -396.36; val LL: -396.49].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 24/350][train LL: -396.02; val LL: -396.41].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 25/350][train LL: -395.86; val LL: -396.44].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 26/350][train LL: -395.93; val LL: -396.29].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 27/350][train LL: -396.12; val LL: -396.49].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 28/350][train LL: -395.86; val LL: -396.09].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 29/350][train LL: -396.05; val LL: -396.20].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 30/350][train LL: -395.68; val LL: -396.18].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 31/350][train LL: -395.78; val LL: -396.10].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 32/350][train LL: -395.91; val LL: -395.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 33/350][train LL: -395.61; val LL: -396.02].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 34/350][train LL: -395.30; val LL: -395.94].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 35/350][train LL: -395.53; val LL: -395.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 36/350][train LL: -395.31; val LL: -395.82].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 37/350][train LL: -395.54; val LL: -395.87].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 38/350][train LL: -395.25; val LL: -395.80].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 39/350][train LL: -394.85; val LL: -395.87].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 40/350][train LL: -395.05; val LL: -395.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 41/350][train LL: -395.05; val LL: -395.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 42/350][train LL: -395.34; val LL: -395.63].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 43/350][train LL: -395.12; val LL: -395.66].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 44/350][train LL: -394.89; val LL: -395.58].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 45/350][train LL: -395.06; val LL: -395.56].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 46/350][train LL: -395.18; val LL: -395.42].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 47/350][train LL: -395.34; val LL: -395.48].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 48/350][train LL: -395.23; val LL: -395.50].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 49/350][train LL: -394.53; val LL: -395.52].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 50/350][train LL: -395.02; val LL: -395.47].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 51/350][train LL: -394.55; val LL: -395.43].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 52/350][train LL: -394.54; val LL: -395.45].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 53/350][train LL: -394.64; val LL: -395.37].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 54/350][train LL: -395.12; val LL: -395.37].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 55/350][train LL: -395.09; val LL: -395.30].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 56/350][train LL: -394.83; val LL: -395.36].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 57/350][train LL: -394.76; val LL: -395.28].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 58/350][train LL: -394.68; val LL: -395.25].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 59/350][train LL: -394.66; val LL: -395.26].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 60/350][train LL: -394.52; val LL: -395.21].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 61/350][train LL: -394.96; val LL: -395.25].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 62/350][train LL: -394.37; val LL: -395.19].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 63/350][train LL: -394.49; val LL: -395.19].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 64/350][train LL: -394.52; val LL: -395.17].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 65/350][train LL: -394.89; val LL: -395.08].....[train forward+backward+step 0.47; val forward 0.00] \n",
      "[Epoch 66/350][train LL: -394.96; val LL: -395.07].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 67/350][train LL: -394.56; val LL: -395.13].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 68/350][train LL: -394.38; val LL: -395.02].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 69/350][train LL: -394.71; val LL: -395.05].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 70/350][train LL: -394.57; val LL: -395.03].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 71/350][train LL: -394.27; val LL: -395.00].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 72/350][train LL: -394.62; val LL: -395.00].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 73/350][train LL: -394.44; val LL: -394.98].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 74/350][train LL: -394.27; val LL: -394.95].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 75/350][train LL: -394.19; val LL: -394.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 76/350][train LL: -394.10; val LL: -394.96].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 77/350][train LL: -394.23; val LL: -394.90].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 78/350][train LL: -394.59; val LL: -394.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 79/350][train LL: -394.78; val LL: -394.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 80/350][train LL: -393.93; val LL: -394.91].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 81/350][train LL: -394.15; val LL: -394.92].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 82/350][train LL: -394.10; val LL: -394.93].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 83/350][train LL: -394.27; val LL: -394.87].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 84/350][train LL: -394.24; val LL: -394.85].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 85/350][train LL: -394.30; val LL: -394.86].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 86/350][train LL: -394.15; val LL: -394.87].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 87/350][train LL: -394.28; val LL: -394.82].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 88/350][train LL: -394.32; val LL: -394.81].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 89/350][train LL: -393.45; val LL: -394.85].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 90/350][train LL: -393.72; val LL: -394.81].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 91/350][train LL: -394.16; val LL: -394.82].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 92/350][train LL: -393.69; val LL: -394.78].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 93/350][train LL: -393.85; val LL: -394.78].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 94/350][train LL: -394.07; val LL: -394.78].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 95/350][train LL: -393.97; val LL: -394.76].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 96/350][train LL: -394.07; val LL: -394.76].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 97/350][train LL: -393.78; val LL: -394.77].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 98/350][train LL: -393.73; val LL: -394.76].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 99/350][train LL: -393.48; val LL: -394.76].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 100/350][train LL: -393.79; val LL: -394.76].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 101/350][train LL: -393.60; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 102/350][train LL: -394.08; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 103/350][train LL: -393.88; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 104/350][train LL: -393.79; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 105/350][train LL: -393.81; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 106/350][train LL: -393.56; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 107/350][train LL: -394.06; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 108/350][train LL: -393.99; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 109/350][train LL: -393.46; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 110/350][train LL: -393.61; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 111/350][train LL: -393.97; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 112/350][train LL: -394.04; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 113/350][train LL: -393.85; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 114/350][train LL: -393.59; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 115/350][train LL: -393.68; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 116/350][train LL: -393.51; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 117/350][train LL: -393.34; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 118/350][train LL: -394.07; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 119/350][train LL: -393.98; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 120/350][train LL: -393.57; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 121/350][train LL: -393.53; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 122/350][train LL: -393.84; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 123/350][train LL: -393.76; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 124/350][train LL: -393.75; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 125/350][train LL: -393.93; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 126/350][train LL: -393.87; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 127/350][train LL: -393.82; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 128/350][train LL: -393.55; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 129/350][train LL: -394.14; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 130/350][train LL: -393.73; val LL: -394.75].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 131/350][train LL: -393.73; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 132/350][train LL: -393.84; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 133/350][train LL: -394.01; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 134/350][train LL: -393.73; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 135/350][train LL: -393.60; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 136/350][train LL: -394.03; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 137/350][train LL: -393.43; val LL: -394.73].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 138/350][train LL: -393.76; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 139/350][train LL: -393.35; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 140/350][train LL: -393.88; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 141/350][train LL: -393.93; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 142/350][train LL: -393.48; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 143/350][train LL: -393.73; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 144/350][train LL: -393.80; val LL: -394.73].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 145/350][train LL: -393.83; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 146/350][train LL: -393.58; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 147/350][train LL: -393.54; val LL: -394.72].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 148/350][train LL: -394.14; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 149/350][train LL: -393.59; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 150/350][train LL: -393.71; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 151/350][train LL: -393.72; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 152/350][train LL: -393.80; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 153/350][train LL: -393.46; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 154/350][train LL: -393.61; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 155/350][train LL: -393.82; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 156/350][train LL: -394.05; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 157/350][train LL: -393.63; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 158/350][train LL: -394.01; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 159/350][train LL: -393.90; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 160/350][train LL: -393.51; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 161/350][train LL: -393.84; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 162/350][train LL: -393.63; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 163/350][train LL: -393.52; val LL: -394.74].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 164/350][train LL: -393.86; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 165/350][train LL: -393.99; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 166/350][train LL: -393.79; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 167/350][train LL: -393.94; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 168/350][train LL: -393.95; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 169/350][train LL: -393.61; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 170/350][train LL: -393.53; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 171/350][train LL: -393.83; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 172/350][train LL: -393.65; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 173/350][train LL: -393.99; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 174/350][train LL: -393.92; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 175/350][train LL: -394.21; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 176/350][train LL: -393.82; val LL: -394.71].....[train forward+backward+step 0.22; val forward 0.00] \n",
      "[Epoch 177/350][train LL: -393.76; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 178/350][train LL: -393.50; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 179/350][train LL: -393.85; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 180/350][train LL: -393.67; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 181/350][train LL: -393.77; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 182/350][train LL: -393.94; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 183/350][train LL: -393.26; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 184/350][train LL: -393.75; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 185/350][train LL: -393.53; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 186/350][train LL: -393.93; val LL: -394.73].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 187/350][train LL: -393.51; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 188/350][train LL: -393.54; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 189/350][train LL: -393.40; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 190/350][train LL: -393.40; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.00] \n",
      "[Epoch 191/350][train LL: -393.74; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.28] \n",
      "[Epoch 192/350][train LL: -393.80; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 193/350][train LL: -393.69; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 194/350][train LL: -393.90; val LL: -394.72].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 195/350][train LL: -393.78; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 196/350][train LL: -393.81; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 197/350][train LL: -393.78; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 198/350][train LL: -393.97; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 199/350][train LL: -394.06; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 200/350][train LL: -393.76; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 201/350][train LL: -393.72; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 202/350][train LL: -393.67; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 203/350][train LL: -393.64; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 204/350][train LL: -393.46; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 205/350][train LL: -393.77; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 206/350][train LL: -393.72; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 207/350][train LL: -393.77; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 208/350][train LL: -393.73; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 209/350][train LL: -393.44; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 210/350][train LL: -393.80; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 211/350][train LL: -393.91; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 212/350][train LL: -394.05; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 213/350][train LL: -393.29; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 214/350][train LL: -393.40; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 215/350][train LL: -394.18; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 216/350][train LL: -393.47; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 217/350][train LL: -393.79; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 218/350][train LL: -393.66; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 219/350][train LL: -393.67; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 220/350][train LL: -393.86; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 221/350][train LL: -393.76; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 222/350][train LL: -393.85; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 223/350][train LL: -393.74; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 224/350][train LL: -394.08; val LL: -394.71].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 225/350][train LL: -393.73; val LL: -394.70].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 226/350][train LL: -393.82; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 227/350][train LL: -393.84; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 228/350][train LL: -393.68; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 229/350][train LL: -393.65; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 230/350][train LL: -393.99; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 231/350][train LL: -393.70; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 232/350][train LL: -393.98; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 233/350][train LL: -393.42; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 234/350][train LL: -393.56; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 235/350][train LL: -393.79; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 236/350][train LL: -393.91; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 237/350][train LL: -393.41; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 238/350][train LL: -393.56; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 239/350][train LL: -394.19; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 240/350][train LL: -393.79; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 241/350][train LL: -393.65; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 242/350][train LL: -393.67; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 243/350][train LL: -393.44; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 244/350][train LL: -393.84; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 245/350][train LL: -393.58; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 246/350][train LL: -393.81; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 247/350][train LL: -393.81; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 248/350][train LL: -393.18; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 249/350][train LL: -393.74; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 250/350][train LL: -393.51; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 251/350][train LL: -393.88; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 252/350][train LL: -393.62; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 253/350][train LL: -393.78; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 254/350][train LL: -393.82; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 255/350][train LL: -393.58; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 256/350][train LL: -393.58; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 257/350][train LL: -393.89; val LL: -394.68].....[train forward+backward+step 0.56; val forward 0.01] \n",
      "[Epoch 258/350][train LL: -393.88; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 259/350][train LL: -393.81; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 260/350][train LL: -393.69; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 261/350][train LL: -393.83; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 262/350][train LL: -393.95; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 263/350][train LL: -393.79; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 264/350][train LL: -393.93; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 265/350][train LL: -393.74; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 266/350][train LL: -393.87; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 267/350][train LL: -393.68; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 268/350][train LL: -393.37; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 269/350][train LL: -393.70; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 270/350][train LL: -393.63; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 271/350][train LL: -393.46; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 272/350][train LL: -393.74; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 273/350][train LL: -393.48; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 274/350][train LL: -393.43; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 275/350][train LL: -393.71; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 276/350][train LL: -393.81; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 277/350][train LL: -393.54; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 278/350][train LL: -393.67; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 279/350][train LL: -393.32; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 280/350][train LL: -394.01; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 281/350][train LL: -393.69; val LL: -394.69].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 282/350][train LL: -393.74; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 283/350][train LL: -393.60; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 284/350][train LL: -393.64; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 285/350][train LL: -393.78; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 286/350][train LL: -394.07; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 287/350][train LL: -393.66; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 288/350][train LL: -393.49; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 289/350][train LL: -393.58; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 290/350][train LL: -393.72; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 291/350][train LL: -393.35; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 292/350][train LL: -393.58; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 293/350][train LL: -393.81; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 294/350][train LL: -393.70; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 295/350][train LL: -393.68; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 296/350][train LL: -393.32; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 297/350][train LL: -393.54; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 298/350][train LL: -393.93; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 299/350][train LL: -393.60; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 300/350][train LL: -393.90; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 301/350][train LL: -393.81; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 302/350][train LL: -393.99; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 303/350][train LL: -393.91; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 304/350][train LL: -394.04; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 305/350][train LL: -393.65; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 306/350][train LL: -393.69; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 307/350][train LL: -393.56; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 308/350][train LL: -393.35; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 309/350][train LL: -393.71; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 310/350][train LL: -393.48; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 311/350][train LL: -393.56; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 312/350][train LL: -393.25; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 313/350][train LL: -393.28; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 314/350][train LL: -393.83; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 315/350][train LL: -393.62; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 316/350][train LL: -393.65; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 317/350][train LL: -393.64; val LL: -394.67].....[train forward+backward+step 0.24; val forward 0.01] \n",
      "[Epoch 318/350][train LL: -393.80; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 319/350][train LL: -393.41; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 320/350][train LL: -393.58; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 321/350][train LL: -393.31; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 322/350][train LL: -393.44; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 323/350][train LL: -393.71; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 324/350][train LL: -393.61; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 325/350][train LL: -393.79; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 326/350][train LL: -393.76; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 327/350][train LL: -393.94; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 328/350][train LL: -393.63; val LL: -394.68].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 329/350][train LL: -393.32; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 330/350][train LL: -393.63; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 331/350][train LL: -393.54; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 332/350][train LL: -393.44; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 333/350][train LL: -393.06; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 334/350][train LL: -393.21; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 335/350][train LL: -393.64; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 336/350][train LL: -393.71; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 337/350][train LL: -393.55; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 338/350][train LL: -393.50; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 339/350][train LL: -393.79; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 340/350][train LL: -393.53; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 341/350][train LL: -393.46; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 342/350][train LL: -393.57; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 343/350][train LL: -393.72; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 344/350][train LL: -393.75; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 345/350][train LL: -393.43; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 346/350][train LL: -393.35; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 347/350][train LL: -393.49; val LL: -394.67].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 348/350][train LL: -393.65; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 349/350][train LL: -393.61; val LL: -394.66].....[train forward+backward+step 0.23; val forward 0.01] \n",
      "[Epoch 350/350][train LL: -393.63; val LL: -394.65].....[train forward+backward+step 0.23; val forward 0.01] \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 350+1):\n",
    "    t0 = time.time()\n",
    "    train_ll = 0.0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "\n",
    "        # Similar to PyTorch optimizers zeroling out the gradients, we zero out the parameter flows\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        lls = pc(x)\n",
    "\n",
    "        # Backward pass\n",
    "        lls.mean().backward()\n",
    "\n",
    "        train_ll += lls.mean().detach().cpu().numpy().item()\n",
    "\n",
    "        # Perform a mini-batch EM step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_ll /= len(train_loader)\n",
    "\n",
    "    t1 = time.time()\n",
    "    test_ll = 0.0\n",
    "    for batch in valid_loader:\n",
    "        x = batch[0].to(pc.device)\n",
    "        lls = pc(x)\n",
    "        test_ll += lls.mean().detach().cpu().numpy().item()\n",
    "\n",
    "    test_ll /= len(valid_loader)\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(f\"[Epoch {epoch}/{350}][train LL: {train_ll:.2f}; val LL: {test_ll:.2f}].....[train forward+backward+step {t1-t0:.2f}; val forward {t2-t1:.2f}] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "juice.save('genetic_pc/test.jpc', pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling 8 TensorCircuit layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 26.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorCircuit(\n",
       "  (input_layer_group): LayerGroup(\n",
       "    (layer_0): InputLayer()\n",
       "  )\n",
       "  (prod_layer_0): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 111 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 111x4 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1x16 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 258 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 258x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_0): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 112x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_1): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 7 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 3 (cuda:0)]\n",
       "          (2): Parameter containing: [torch.int64 of size 4 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 7x8 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 3x16 (cuda:0)]\n",
       "          (2): Parameter containing: [torch.int64 of size 4x64 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 155 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 155x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_1): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 14x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_2): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1x2 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1x128 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 67 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 67x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_2): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_3): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1x4 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1x64 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 51 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 51x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_3): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_4): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1x2 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1x64 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 44 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 44x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_4): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_5): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_cids): FastParamList(\n",
       "          (0): Parameter containing: [torch.int64 of size 1x4 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.int64 of size 1x512 (cuda:0)]\n",
       "      )\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 358 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 358x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_5): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 2x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (prod_layer_6): LayerGroup(\n",
       "    (layer_0): ProdLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x8 (cuda:0)])\n",
       "      (partitioned_u_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 6 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 6x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       "  (sum_layer_6): LayerGroup(\n",
       "    (layer_0): SumLayer(\n",
       "      (partitioned_nids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1 (cuda:0)])\n",
       "      (partitioned_cids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x128 (cuda:0)])\n",
       "      (partitioned_pids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x128 (cuda:0)])\n",
       "      (partitioned_pfids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x128 (cuda:0)])\n",
       "      (partitioned_chids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1 (cuda:0)])\n",
       "      (partitioned_parids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x1 (cuda:0)])\n",
       "      (partitioned_parpids): FastParamList(  (0): Parameter containing: [torch.int64 of size 1x1 (cuda:0)])\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = juice.load('genetic_pc/test.jpc')\n",
    "pc = juice.compile(ns)\n",
    "pc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyjuice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
